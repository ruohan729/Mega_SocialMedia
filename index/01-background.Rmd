<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called background.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from 01-background.Rmd.
-->

<!--
The {#background} text after the chapter declaration will allow us to link throughout the document back to the beginning of of the background section.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Background {#background .unnumbered}

## Social Media Variables {.unnumbered}
For this forecasting project, we focus on collecting social media data on relevant topics. These relevant topics are Coca-Cola, Pepsi, McDonald’s, Taco Bell, and “jobs”.  Coca-Cola is selected because it is their product’s demand that Scholle IPN is interested in forecasting. Pepsi is selected due to its position as the main competitor for Coca-Cola.  Meanwhile, McDonald’s and Taco Bell are chosen because they are the top quick service restaurant partners for Coca-Cola and Pepsi based on 2018 annual sales revenue. The topic ‘jobs’ is selected to gather job-related tweets intended to capture economic activity in the United States and Canada.  Relationships between social media activity on these topics and the quantity of Coca-Cola bag ordered can be useful information for our forecasting models.
Twitter data is a combination of user-level tweets and company-level tweets, while Google Trend data is monthly trend value for our selected terms.  User-level tweets are Twitter posts from regular online consumers tweeting about Coca-Cola, Pepsi, McDonald’s, Taco Bell, and “jobs.”  Company-level tweets are Twitter posts by the official Twitter accounts of Coca-Cola, Pepsi, McDonald’s, and Taco Bell.  Meanwhile, Google Trends is a popularity measure for Coca-Cola and other relevant terms based on their search frequency over time [insert reference]. 

## Sentiment Analysis {.unnumbered}
One method of quantifying text-based social media data for our forecasting models is by implementing sentiment analysis.  This is a technique in natural language processing that we use for each tweet to generate a numerical value signifying whether consumers have a positive or negative outlook on Coca-Cola, Pepsi, McDonald’s, Taco Bell, or 'jobs'.  We then average sentiment scores by month for each relevant term, and use this as an additional predictor to forecast Coca-Cola bag orders.  For this project, we only calculate sentiment scores for user-generated tweets because we assume that tweets generated by the official company accounts are positively biased towards their own brand.  Prior to conducting sentiment analysis, text processing steps must be conducted on tweets. The following steps were conducted on the tweets:

* Remove stop words on tweets
* Tokenize tweets
* Lemmatize tweets   

The R package sentimentR was used to calculate sentiment scores for each tweet. This package takes into account additional information such as valence shifters and de-amplifiers resulting in a more accurate sentiment score (see Appendix A).  The sentiment scores for each selected term’s collective tweets per month is averaged at the monthly level to generate the monthly average sentiment variable.  

## Stationarity and Differencing {.unnumbered}
An important step to consider when forecasting is to remove trends and seasonality from variables in order to make it stationary.  When time series data is stationary, it displays a stable mean and stable variance over time [insert reference].  This means that it is less likely to produce spurious relationships and misleading results.  Statistical tests (KPSS test) are conducted on each variable to determine its level of stationarity (see Appendix A).  After testing, it was determined that the dependent variable, monthly quantity of Coca-Cola bag orders, is stationary.  This means that this variable requires no further transformations.  However, the independent variables show varying results and require additional processing.  One way of transforming time series data to become stationary is the method of differencing.  Differencing is the method of subtracting the value of the current time step from the value of previous time step(s).  This method was applied to all social media variables to ensure that they were all stationary.  The visual below demonstrates how the method of differencing is able to remove trends from the monthly total user tweets.  The top half of the visual are time plots of the pre-differenced variables, while the bottom are time plots of the differenced variables.

```{r differencing, out.width="200px", fig.align="center", out.extra="angle = 0, scale=2.1", fig.cap="Differenced social media variables", echo=FALSE}
include_graphics(path = "figure/differencing.jpg")
```

## Dimension Reduction {.unnumbered}
When building forecasting models, it is important to be aware of the level of complexity of these models.  In this project, we collect Twitter data with over a hundred features for a single tweet (tweet text, user profile data, etc.).  Using all of these variables will make our forecasting models highly complex and likely result in poor predictions.  Fortunately, the scope of this project limits tweet information to only a tweet’s text, number of likes, number of retweets, and number of replies.  However, the complexity of this project (relevant topics, social media data type, social media source) still leaves us with 46 total independent variables per observation (see Data section).  We will use two main approaches in this project to further reduce our social media variable’s dimensions.

#### Method A - Principal Component Analysis {.unnumbered} 
The first method we will use to reduce the dimensionality of our social media variables is principal component analysis (PCA).  PCA uses an orthogonal transformation of our variables into linearly uncorrelated variables called principal components.   The main idea is that the majority of the variance explained will be concentrated on a limited number of principal components.  This allows us to discard the principal component variables that provide little additional information.  This approach further reduces the overall dimension of our original set of variables.  When performing this technique with our social media variables, we are able to observe the “elbow feature” at number of principal component (n) = 7 .  This tells us that only the first seven principal components is required to explain most of the total variance (97%) of our variables. By using PCA, we are able to reduce our overall social media variables from 46 to 7.

```{r pca, out.width="200px", fig.align="center", out.extra="angle = 0, scale=2.1", fig.cap="Variable Explained vs. Principal Components Plot", echo=FALSE}
include_graphics(path = "figure/pca.png")
```

#### Method B - Cross Correlation {.unnumbered}
The second method we employ to reduce our total features is by testing our independent variables for cross correlation with the dependent variable.  Mainly, this approach will inform us how many months in advance a social media variable can lead to an increase or decrease in Coca-Cola bag orders. For example, consider when a social media variable was found to be significantly positively correlated with Coca-Cola bag orders at lag t-1.  If this social media variable has a positive value for the current month, then an increase in Coca-Cola bag orders can be expected the following month. In this project, we check for cross correlation on each independent variable up to six months prior (t-6). We will do this on the original variables as well as the principal component variables. Using the cross correlation approach, we identified eight lags from the social media variables that were cross correlated with Coca-Cola bag orders. Below is a table of these social media variables with their specified lags listed. 

--------------------------------------------------------------
  Social Media Variable                    Significant Lag   
--------------------------------  ---------------------------- 
  Coca-Cola  Account tweets                    t-1                     
  
  Taco Bell  Account tweets                    t-1                       
  
  Job Google Trend                             t-2                        
  
  McDonald’s Google Trend                      t-2
  
  McDonald’s Account replies                   t-5
  
  Taco Bell Google Trend                       t-5
  
  McDonald’s Google Trend                      t-5
  
  Pepsi Account tweets                         t-6
--------------------------------  ---------------------------- 
Table: (\#tab:inher) Social Media Variables with Specified Lags

In addition, we identified seven lags from principal components that are cross correlated with Coca-Cola bag orders.  Below is a table of these principal components with their specified lags listed. 

----------------------------------------------------------
  Principal Component             Significant Lag   
------------------------- -------------------------------- 
           PC7                            t-1                     
  
           PC6                            t-2                          
  
           PC4                            t-2                         
  
           PC6                            t-3
  
           PC4                            t-3
  
           PC3                            t-5
  
           PC3                            t-6
------------------------- ----------------------------------------- 
Table: (\#tab:inher) Principal Components with Specified Lags

## Ensemble Modeling {.unnumbered}
A variety of different machine learning models is used to forecast future Coca-Cola bag orders (see Modeling Framework).  In addition to these machine learning models, this project demonstrate the strength of the ensemble model approach. The main assumption to ensemble modeling is that combining all lower-level models will result in a more accurate, overall model. An ensemble model is able to highlight the strength of each individual model and account for each model’s weaknesses.  The ensemble model approach will be used in this project to produce the best model.


\newpage
