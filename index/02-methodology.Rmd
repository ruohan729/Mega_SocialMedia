# Methodology {#methodology .unnumbered}

## Data {#methodology-data .unnumbered}

The data used to predict Coca-Cola bag orders will come from three distinct sources: Scholle IPN, Twitter, and Google Trends.  The aggregation of all relevant data will be at the monthly level with a date range from October 2009 to October 2018.  This will ensure that the data will have enough observations for our forecasting models.  The dependent variable for this project will be the monthly total bag orders from Coca-Cola, which will be calculated using Scholle IPN’s internal sales data.  The independent variables are social media variables collected from Twitter and Google Trends.
Twitter is a widely used social media platform in the United States and Canada that was founded in 2006.  This platform will allow us to get a feeling about users and their opinions on Coca-Cola, Pepsi, McDonald’s, Taco Bell, and ‘jobs’.  Most importantly, Twitter gives users access to data elements about a tweet necessary for this project, including the date a tweet was posted and the reactions a tweet received (i.e. number of likes, replies, and retweets).  Twitter’s years of existence, popularity, and data features make it an ideal social media data source for this project.  However, Twitter does have a number of limitations.  One of the main disadvantages of using Twitter data is its high volume and high dimensionality.  Twitter receives millions of tweets a day that has information about the actual tweet (number of likes, replies, etc.), the user who posted the tweet (username, location, etc.) and the users who interact with the tweet (replied to tweet, username, etc.).  A well defined scope will limit the amount of tweets to be collected and will allow us to prepare for any data storage and computational needs.	  
Google Trends will give us an opportunity to see how frequently internet users search for Coca-Cola, Pepsi, McDonald’s, Pepsi, and ‘jobs’ in Google.  The main advantage of Google Trends is the ease in which we are able to collect this data.  Google Trends allow users to select aggregate level (monthly, annual), location, and a date range for each query.  The main disadvantage of Google Trends is that the raw data used to generate the trend value is not available.  This makes it challenging to validate unusual trends in a search query. 
For more information on the specific variables, please refer to the Social Media Variables section. For more information on how the social media data were collected, please refer to Appendix A.  Below is a summary table of the monthly social media variables:


```{r data}
data(ToothGrowth)
```

## Descriptive analyses {#methodology-descriptive .unnumbered} 

**Identification of the methodologies that would provide a view into the data. These may include highlighting the relationships that are important for the research, validating assumptions with respect to important metrics, and providing evidence that substantiates the way data is analyzed.**



```{r datatable, echo=TRUE}
growth_short <- rbind(ToothGrowth[1:7,], ToothGrowth[26:34,], ToothGrowth[53:60,])
kable(growth_short, align = "r", format = "latex", 
      longtable = TRUE, caption = "Tooth Growth")
```

Table: \@ref(tab:datatable): Head, center, and tail of _Tooth Growth_ data.


\newpage

## Modeling Framework {#methodology-modeling .unnumbered}

**Justification of model(s) selected. Identification of dependent and independent variables, per model as well as variable transformations. Feature extraction (if applicable). Discussion of model(s) functional form. Assumptions of model(s) and ways of insuring that assumptions are observed or tested. Assessing model(s) performance and validation.**

Transform `dose` into a `factor`. Only three dosage levels are present.

```{r transform}
data(ToothGrowth)
colnames(ToothGrowth) <- c("length", "supplement", "dose")
ToothGrowth$dose <- as.factor(ToothGrowth$dose)
```
 
We are most interested in discovering which treatment leads to the optimal tooth growth.
In this vein, we use `aggregate` function to transform our data and compute the average tooth `length` by both `supplement` type  and `dose` size.


```{r group, warning=FALSE}
groupedTooth <- aggregate(ToothGrowth, by=ToothGrowth[,2:3], FUN=mean)[,1:3]

kable(groupedTooth, align = "r", caption = "Average tooth length",
      format = "latex", longtable = TRUE)
```                   



\newpage

## Math and Science notation  {#math-sci .unnumbered}
<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

\TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read \LaTeX\ code directly.

Get around math mode's automatic italicizing in LaTeX by using the argument `$\mathrm{formula here}$`, with your formula inside the curly brackets.  (Notice the use of the backticks here which enclose text that acts as code.)

So, $\mathrm{Fe_2^{2+}Cr_2O_4}$ is written `$\mathrm{Fe_2^{2+}Cr_2O_4}$`.

The \noindent command below does what you'd expect:  it forces the current line/paragraph to not indent. See below and examples of commonly used symbols:

\noindent Exponent or Superscript written as `$x^2$` becomes $x^2$

\noindent Subscript written as `$x_1$` becomes $x_1$

\noindent Infinity written as `$\infty$` becomes $\infty$

\noindent alpha written as `$\alpha$` becomes $\alpha$

\noindent beta written as `$\beta$` becomes $\beta$

\noindent delta written as `$\delta$` becomes $\delta$

\noindent epsilon written as `$\epsilon$` becomes $\epsilon$

\noindent sigma written `$\sum_{i=1}^n f(x)$` becomes $\sum_{i=1}^n f(x)$

### Math Examples {.unnumbered}

An Ordinary Least Squares model, from _Introductory Econometrics, 6th edition_ by Jeffrey M. Wooldridge, page 27. 

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$ 

An infinite distributed lag (IDL) time series model, by Wooldridge, page 633.

$$ y_t = \alpha + \delta_0 z_t + \delta_1 z_{t-1} + \delta_2 z_{t-2} \ldots + \epsilon_t$$ 

A vector autoregressive (VAR) model, by Wooldridge, page 657. 

$$ y_t = \delta_0 + \alpha_1 y_{t-1} + \gamma_1 z_{t-1} + \alpha_2 y_{t-2} + \gamma_2 z_{t-2} \ldots,$$ 
\newpage
ddfdf
Determinant of a square matrix: 


$$\det\left|\,\begin{matrix}%
c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr
c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr
c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr
\,\vdots\hfill&\,\vdots\hfill&
  \,\vdots\hfill&&\,\vdots\hfill\cr
c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr
\end{matrix}\right|>0$$ 

\bigskip

A regularization problem solved by Jerome Friedman, Trevor Hastie, Rob Tibshirani and Noah Simon, implemented in the [R package `glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html). 


$$ \min_{\beta_0,\beta} \frac{1}{N}\sum_{i=1}^N w_il(y_i,\beta_0+\beta^Tx_i)+\lambda \left[(1-\alpha) ||\beta||_2^2/2+\alpha||\beta||_1\right]$$ 

\bigskip

From Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and Engineering, page 54. 

$$\int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0, \qquad\quad i=1,2,3.$$ 

\bigskip
  
From Lapidus and Pindar, page 145. 

$$\int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big) = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big).$$ 



### Additional R Markdown and bookdown resources {.unnumbered}

- _Bookdown_ Online Book - <https://bookdown.org/yihui/bookdown/>

- _Markdown_ Info Sheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet>

- _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf>
