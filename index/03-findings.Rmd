# Findings {#findings .unnumbered}


## Baseline Model Results  {.unnumbered}

Before we tested for the impact of social media variables in forecasting Coca-Cola bag orders, we first established baseline metrics.  We used a seasonal Arima model incorporating only the Scholle IPN's Coca-Cola bad order data to establish our baseline model.  Our analysis found that the best seasonal Arima model had an order of (3,0,1)(1,0,0)[12].   Figure 11 is a plot of the forecast horizon against each model’s sMAPE values.

```{r SMAPEvsHorizon, out.width="200px", fig.align="center",out.extra="angle = 0, scale=2.1", fig.cap="sMAPE vs. Horizon Plot", echo=FALSE}
include_graphics(path = "figure/SMAPEvsHorizon.png")
```

Figure 11 clearly showed how prediction results flatten out after a certain point in time for each sliding window model (2, 4 and 6 years).  For this baseline model, the prediction results flattened out after twelve months.  In other words, this baseline model provides the most value with a forecast horizon capped at twelve months.  In addition, figure 11 clearly showed that the two-year window model is considerably worse than either the four-year and six-year sliding window models. Overall, there was no considerable improvement in sMAPE values from the four-year window model to the six-year window model. Since there was not much improvement observed when using six years worth of data over four years, the baseline model selected is the seasonal Arima model trained on four years worth of observations.

----------------------------------------------------------------------------------------------------------
  Sliding Window                   sMAPE             RMSE            Mean Accuracy       Bias (%)
-------------------------- ------------------- ------------------ ------------------- --------------------
2 Year                            12.79%          1,066,599            86.56%             50.00% 

4 Year                            13.93%          1,215,263            86.64%             38.89% 

6 Year                            14.14%          1,066,599            86.49%             38.89% 
-------------------------- ------------------- ------------------ ------------------- --------------------

## Regression with ARIMA Errors Results  {.unnumbered}
By evaluating the residuals and metrics, we found that regression with Arima errors produced the best results using the lagged principal components.  With a sMAPE value of 9.76% and RMSE value of 888,544, which was an improvement compared to the Scholle Coca-Cola baseline model.

-------------------------------------------------------------------------------------------------------------------
Social Media Variable                       sMAPE            RMSE            Mean Accuracy           Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Variables                        
Principal Components                         9.76%         888,544             90.20%                33.00%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

## XGBoost  Results  {.unnumbered}
For the XGBoost model, we introduced a month of year variable to allow the model to learn seasonality.  The XGBoost performed better using the lagged social media variables.  This model produced a sMAPE value of 6.21% and RMSE value of 597,983, which is the best performing challenger model.

--------------------------------------------------------------------------------------------------------------------
Social Media Variable                     sMAPE             RMSE            Mean Accuracy           Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Variables                      6.21%            597,983            93.88%                27.78%
Principal Components                       7.27%            701,980            93.01%                38.89%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

After running the model, we are able to rank the features based on importance (see below).
```{r FeatureImportance, out.width="200px", fig.align="center",out.extra="angle = 0, scale=2.1", fig.cap="XGBoost Feature Importance", echo=FALSE}
include_graphics(path = "figure/FeatureImportance.png")
```

## Long Short-Term Memory Results  {.unnumbered}

For the LSTM model, we also introduced a month of year variable to allow the model to learn seasonality.  Similar to XGBoost, this model experienced better performance using the lagged social media variables.  The LSTM model using lagged social media variabled produced a sMAPE value of 12.3% and RMSE value of 1,023,904. Among all the challenger models, the LSTM model had the worst performing metrics.

```{r image17, out.width="200px", fig.align="center",out.extra="angle = 0, scale=1.5", fig.cap="LSTM Epoch vs. Loss Function", echo=FALSE}
include_graphics(path = "figure/image17.png")
```

--------------------------------------------------------------------------------------------------------------------
 Social Media Variable                  sMAPE                RMSE            Mean Accuracy          Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Variables                   12.30%              1,023,904           87.56%                38.00%

Principal Components                    13.46%              1,145,561           86.82%                38.89%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

## Ensemble Model  Results  {.unnumbered}
After building challenger models that utilized social media variables, we constructed ensemble models.  

--------------------------------------------------------------------------------------------------------------------
 Model                                  sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Scholle Baseline Model                   7.43%            667,832             92.84%               27.78%

Linear Regression                        5.82%            541,989             94.17%               55.00%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

Ensembling the three challenger models we built produced more accurate Coca-Cola bag order predictions. The results show that the random forest ensemble model performs the best according  to the sMAPE and RMSE metrics.  However, since the linear model produced the most stable results, this will be selected as the champion model. Figure 14 is a visualizing of our challenger model forecasts against the actual quantity of Coca-Cola bag orders.

```{r timeplot, out.width="200px", fig.align="center",out.extra="angle = 0, scale=2.5", fig.cap="Model Predictions vs. Forecast Window", echo=FALSE}
include_graphics(path = "figure/timeplot.png")
```

## Coca-Cola Ensemble Model  Results  {.unnumbered}

To further demonstrate the strength of ensemble modeling, this method was also used to combine models generated by other Scholle IPN capstone teams.  Other teams were solving the same business problem by using a different set of covariates (related products and economy).  The idea is that ensembling all the team’s models will compensate for each model’s weaknesses and produce a strong overall model.   The final model generated incremental gains from our best ensemble model. The results are outlined below.

--------------------------------------------------------------------------------------------------------------------
            Model                       sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Scholle Baseline Model                  7.43%            667,832             92.84%               27.78%

Social Media Ensemble                   5.76%            545,211             94.22%               50.00%

Linear Regression                       1.05%            93,662              98.95%               44.44%

Random Forest                           1.34%            119,437             98.64%               50.00%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

```{r coketeams, out.width="200px", fig.align="center",out.extra="angle = 0, scale=2.5", fig.cap="Coke Ensemble Model Predictions vs. Forecast Window", echo=FALSE}
include_graphics(path = "figure/coketeams.png")
```


