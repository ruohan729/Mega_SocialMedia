# Findings {#findings .unnumbered}


## Baseline Model Results  {.unnumbered}

Before we test the impact of social media variables in forecasting Coca-Cola bag orders, we must first establish baseline metrics.  We will use a seasonal Arima model that uses only the dependent variable to establish our baseline model.  Our analysis found that the best seasonal Arima model had an order of (3,0,1)(1,0,0)[12].  Cross validation with a sliding window of two-years, four-years, and six-years was then conducted.  The purpose of cross validation is to establish the metric that the challenger models will be compared against and to find the point in the forecast horizon in which the predictions “flatten out”.  The chart below is a plot of the forecast horizon against each model’s sMAPE values.

[INSERT FIGURE]

It is clear that the two-year window model is considerably worse than either the four-year and six-year sliding window models.  When you look at a plot of each model against the average sMAPE value for each forecast window, you can see that the sMAPE values flatten out at the four year mark.  There is no considerable improvement in sMAPE values between the four-year window model and the six-year window mode. Therefore, the best baseline model is the four-year sliding window model.

----------------------------------------------------------------------------------------------------------
  Sliding Window                   sMAPE             RMSE            Mean Accuracy       Bias (%)
-------------------------- ------------------- ------------------ ------------------- --------------------
2 Year                            12.79%          1,066,599            86.56%             50.00% 

4 Year                            13.93%          1,215,263            86.64%             38.89% 

6 Year                            14.14%          1,066,599            86.49%             38.89% 
-------------------------- ------------------- ------------------ ------------------- --------------------

## Regression with ARIMA Errors Results  {.unnumbered}
By evaluating the residuals and metrics above, we found that manually passing the ARIMA order (3,0,1)(1,0,0) [12] to the model gave us the best results.  The residuals after the model demonstrates the white noise feature, meaning that the model has captured all the information from the time series data.  When reviewing the forecast based on the regression with arima (3,0,1)(1,0,0)[12] model,  the results are relatively conservative.

--------------------------------------------------------------------------------------------------------------------
 Social Media Var                       sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Social Media Variables       37.79%         3,107,383           65.05%              55.55%

Principal Components                     19.11%         1,381,487           80.33%              38.89%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

## XGBoost  Results  {.unnumbered}
For the XGBoost model, we introduced a month of year variable to allow the model to learn seasonality.  XGBoost was implemented in Python using the parameters below before using grid search to identify the best tuning parameter.

After running the model, we are able to rank the features based on importance (see below.)
[INSERT FIGURE]

--------------------------------------------------------------------------------------------------------------------
 Social Media Var                       sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Social Media Variables       6.21%              597,983           93.88%                27.78%

Principal Components                     7.27%              701,980           93.01%                38.89%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

## Long Short-Term Memory Results  {.unnumbered}
We  implemented LSTM in Python using the same set of variables as the previous models. Below is a model summary of LSTM prior to implementation.
[INSERT FIGURE]

Tuning the parameters is a vital part when deploying any Neural Network models, and LSTM is no exception. Listed  below are  a  few key parameters we tweaked: 

* Epochs - One Epoch is when an entire dataset is passed forward and backward through the neural network only once. Since one epoch is too big to feed to the computer at once we divide it in several smaller batches. (we need to quote this!)
* Optimizer - adam
* Loss function - mean squared error
Activation function - since we scaled the train and test sets beforehand, we chose to leave the activation function default. 
* Dropout - we had 4 dropout layers embedded in the model. Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting. 

[INSERT FIGURE]

--------------------------------------------------------------------------------------------------------------------
 Social Media Var                       sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Differenced Social Media Variables       11.8%              994,364           88.47%                44.4%

Principal Components                     38.89%           1,114,561           86.82%                38.89%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

## Ensemble Model  Results  {.unnumbered}
After building challenger models that utilize social media variables, we will now construct an ensemble model.   Ensembling the three challenger models we built should produce more accurate Coca-Cola bag order predictions.  We will use several ensemble models and compare these results against each individual challenger model.  The ensemble models we will build are ‘average of all models’, linear regression, and random forest. 

The average of all predictions ensemble model is the most interpretable of all the ensemble models as it simply takes the average of all predicted values among all challenger models.  Meanwhile, the linear model ensemble model will assign a weight to each challenger model’s results.  The more that a model contributes to the predicted results, the more weight the linear model will assign it.  Finally, the random forest ensemble model is the most complex among the three.  This model will capture how each challenger model’s specific weaknesses are in prediction.  For the linear regression and random forest models, the predictions of the three challenger models will be used as training observations.

--------------------------------------------------------------------------------------------------------------------
 Model                                   sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Scholle Baseline Model                  7.43%            667,832             92.84%               27.78%

Average of all Models                   8.41%            809,139             91.53%               33.33%

Linear Regression                       5.76%            545,211             94.22%               50.00%

Random Forest                           3.57%            356,563             96.37%               44.44%
----------------------------------- ---------------- ------------------ --------------------- ---------------------

The results show that the random forest ensemble model performs the best according  to the sMAPE and RMSE metrics.  However, since the linear model produced acceptable results and is the simpler ensemble model, this will be selected as the champion model. Below is a graph visualizing our individual model forecasts against the actual quantity (black).

[INSERT FIGURE]

## Coca-Cola Ensemble Model  Results  {.unnumbered}

To further demonstrate the strength of ensemble modeling, this method was also used to combine the models generated by other capstone teams.  Other teams were solving the same business problem, but using a different set of covariates (related products and economy).  The idea is that ensembling all the team’s models will compensate for each model’s weaknesses and produce a strong overall model.   The final model generated incremental gains from our best ensemble model. The results are outlined in the next section.

--------------------------------------------------------------------------------------------------------------------
            Model                       sMAPE                RMSE            Mean Accuracy       Bias (%)
----------------------------------- ---------------- ------------------ --------------------- ---------------------
Scholle Baseline Model                  7.43%            667,832             92.84%               27.78%

Linear Regression (social media)        5.76%            545,211             94.22%               50.00%

Linear Regression                       1.05%            93,662              98.95%               44.44%

Random Forest                           1.34%            119,437             98.64%               50.00%
----------------------------------- ---------------- ------------------ --------------------- ---------------------



